{"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/fireholder/Hung-yi-Lee-Machine-Learning-2020-Homework/blob/main/Note-7/hw7_Weight_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text","cell_id":"00000-dd857651-357e-41fd-8f20-af6013760125"}},{"cell_type":"markdown","source":"# Homework 7 - Network Compression (Weight Quantization)\n\n> Author: Arvin Liu (b05902127@ntu.edu.tw)\n\n若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com","metadata":{"id":"xXMVwO8CRbqo","cell_id":"00001-c0a45b5b-47c5-4333-8c4c-b3f4e6169e9a"}},{"cell_type":"markdown","source":"# Readme\n\n\nHW7的任務是模型壓縮 - Neural Network Compression。\n\nCompression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n\n* 知識蒸餾 Knowledge Distillation\n* 網路剪枝 Network Pruning\n* 用少量參數來做CNN Architecture Design\n* 參數量化 Weight Quantization\n\n在這個notebook中我們會介紹非常簡單的Weight Quantization，\n而我們有提供已經做完Knowledge Distillation的小model來做Quantization。\n\n* Model架構 / Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n* 下載已經train好的小model(0.99M): https://drive.google.com/open?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\n  * 參數為 base=16, width_mult=1 (default)\n\n\n## Weight Quantization\n<img src=\"https://i.imgur.com/SMsaiAo.png\" width=\"500px\">\n\n我們這邊會示範如何實作第一條: Using less bits to represent a value。\n\n## 好的Quantization很重要。\n這邊提供一些TA的數據供各位參考。\n\n|bit|state_dict size|accuracy|\n|-|-|-|\n|32|1047430 Bytes|0.81315|\n|16|522958 Bytes|0.81347|\n|8|268472 Bytes|0.80791|\n|7|268472 Bytes|0.80791|\n\n\n## Byte Cost\n根據[torch的官方手冊](https://pytorch.org/docs/stable/tensors.html)，我們知道torch.FloatTensor預設是32-bit，也就是佔了4byte的空間，而FloatTensor系列最低可以容忍的是16-bit。\n\n為了方便操作，我們之後會將state_dict轉成numpy array做事。\n因此我們可以先看看numpy有甚麼樣的type可以使用。\n![](https://i.imgur.com/3N7tiEc.png)\n而我們發現numpy最低有float16可以使用，因此我們可以直接靠轉型將32-bit的tensor轉換成16-bit的ndarray存起來。","metadata":{"id":"9Hwg2LOQRaee","cell_id":"00002-6c5b9b06-5b31-4deb-afe4-fa43640c182b"}},{"cell_type":"markdown","source":"## Low Rank Approximation\n\n* 减小参数量","metadata":{"tags":[],"cell_id":"00003-898b58fe-0377-417c-99ae-bec77fbd495e"}},{"cell_type":"markdown","source":"# Read state_dict\n\n下載我們已經train好的小model的state_dict進行測試。","metadata":{"id":"O6PTWn7LdvMn","cell_id":"00003-cbada8c8-1b5b-43a3-9282-58c8cba72670"}},{"cell_type":"code","metadata":{"id":"XRe3T_Uwd29U","outputId":"33a396d4-1277-4352-a028-de7eee9825d5","colab":{"base_uri":"https://localhost:8080/","height":118},"cell_id":"00004-0e11f6f7-649f-4389-8d8d-8a8acf88acd0"},"source":"!gdown --id '12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL' --output student_custom_small.bin\n\nimport os\nimport torch\n\nprint(f\"\\noriginal cost: {os.stat('student_custom_small.bin').st_size} bytes.\")\nparams = torch.load('student_custom_small.bin')","execution_count":null,"outputs":[{"output_type":"stream","text":"Downloading...\nFrom: https://drive.google.com/uc?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\nTo: /content/student_custom_small.bin\n\r  0% 0.00/1.05M [00:00<?, ?B/s]\r100% 1.05M/1.05M [00:00<00:00, 69.9MB/s]\n\noriginal cost: 1047430 bytes.\n","name":"stdout"}]},{"cell_type":"markdown","source":"# 32-bit Tensor -> 16-bit ","metadata":{"id":"SXD0_G5McRGt","cell_id":"00005-e37e7020-188a-4456-adf4-bcee56bf3d43"}},{"cell_type":"code","metadata":{"id":"OIV0BzszcQg8","outputId":"6b97bce2-0a1a-400d-ae65-5fdfce1c9fed","colab":{"base_uri":"https://localhost:8080/","height":34},"cell_id":"00006-6f3ef407-f15a-4339-8ad3-196b3e13c2c8"},"source":"import numpy as np\nimport pickle\n\ndef encode16(params, fname):\n    '''將params壓縮成16-bit後輸出到fname。\n\n    Args:\n      params: model的state_dict。\n      fname: 壓縮後輸出的檔名。\n    '''\n\n    custom_dict = {}\n    for (name, param) in params.items():\n        param = np.float64(param.cpu().numpy())\n        # 有些東西不屬於ndarray，只是一個數字，這個時候我們就不用壓縮。\n        if type(param) == np.ndarray:\n            custom_dict[name] = np.float16(param)\n        else:\n            custom_dict[name] = param\n\n    pickle.dump(custom_dict, open(fname, 'wb'))\n\n\ndef decode16(fname):\n    '''從fname讀取各個params，將其從16-bit還原回torch.tensor後存進state_dict內。\n\n    Args:\n      fname: 壓縮後的檔名。\n    '''\n\n    params = pickle.load(open(fname, 'rb'))\n    custom_dict = {}\n    for (name, param) in params.items():\n        param = torch.tensor(param)\n        custom_dict[name] = param\n\n    return custom_dict\n\n\nencode16(params, '16_bit_model.pkl')\nprint(f\"16-bit cost: {os.stat('16_bit_model.pkl').st_size} bytes.\")","execution_count":null,"outputs":[{"output_type":"stream","text":"16-bit cost: 522958 bytes.\n","name":"stdout"}]},{"cell_type":"markdown","source":"# 32-bit Tensor -> 8-bit (OPTIONAL)\n\n這邊提供轉成8-bit的方法，僅供大家參考。\n因為沒有8-bit的float，所以我們先對每個weight記錄最小值和最大值，進行min-max正規化後乘上$2^8-1$在四捨五入，就可以用np.uint8存取了。\n\n$W' = round(\\frac{W - \\min(W)}{\\max(W) - \\min(W)} \\times (2^8 - 1)$)\n\n\n\n> 至於能不能轉成更低的形式，例如4-bit呢? 當然可以，待你實作。","metadata":{"id":"Mv5PtFoWgIFb","cell_id":"00007-498bdb83-f5c4-4f84-acfb-8b315beb7e31"}},{"cell_type":"code","metadata":{"id":"2vh9Pn-3hZEN","outputId":"91513e99-2e7e-4736-eea9-73bb2a624523","colab":{"base_uri":"https://localhost:8080/","height":34},"cell_id":"00008-eb001f28-ec04-4af6-bf16-52b4d9404f53"},"source":"def encode8(params, fname):\n    custom_dict = {}\n    for (name, param) in params.items():\n        param = np.float64(param.cpu().numpy())\n        if type(param) == np.ndarray:\n            min_val = np.min(param)\n            max_val = np.max(param)\n            param = np.round((param - min_val) / (max_val - min_val) * 255)\n            param = np.uint8(param)\n            custom_dict[name] = (min_val, max_val, param)\n        else:\n            custom_dict[name] = param\n\n    pickle.dump(custom_dict, open(fname, 'wb'))\n\n\ndef decode8(fname):\n    params = pickle.load(open(fname, 'rb'))\n    custom_dict = {}\n    for (name, param) in params.items():\n        if type(param) == tuple:\n            min_val, max_val, param = param\n            param = np.float64(param)\n            param = (param / 255 * (max_val - min_val)) + min_val\n            param = torch.tensor(param)\n        else:\n            param = torch.tensor(param)\n\n        custom_dict[name] = param\n\n    return custom_dict\n\nencode8(params, '8_bit_model.pkl')\nprint(f\"8-bit cost: {os.stat('8_bit_model.pkl').st_size} bytes.\")","execution_count":null,"outputs":[{"output_type":"stream","text":"8-bit cost: 268471 bytes.\n","name":"stdout"}]},{"cell_type":"markdown","source":"# Q&A\n\n有任何問題Network Compression的問題可以寄信到b05902127@ntu.edu.tw。\n\n時間允許的話我會更新在這裡。","metadata":{"id":"m0M7JRXSjQwa","cell_id":"00009-5a4c96aa-b8f6-49cf-8b93-0804c97f5216"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw7_Weight_Quantization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","deepnote_notebook_id":"6fa6414e-5f01-481b-ba94-5af975114c95","deepnote_execution_queue":[]}}